{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SNOTEL Processing for pySUMMA Modeling\n",
    "\n",
    "This notebook pulls SNOTEL data via metloom to create a meteorological forcing file for pysumma. 7 input meteorological variables are needed at hourly (if using SNOTEL data, hourly is highest temporal resolution possible) timesteps: air temperature, precipitation, incoming shortwave radiation, incoming longwave radiation, air pressure, relative humidity, and wind speed. \n",
    "\n",
    "Temperature used is observed from SNOTEL with the *Currier et al. (2017)* voltage issue correction. Precipitation used is observed from SNOTEL - be wary of undercatch for upper elevation SNOTEL sites in windier locations, check to ensure accumulated precip > max SWE. Incoming shortwave radiation is empirically derived using latitude, elevation, and time of year to calculate clear sky radiation and the diurnal temperature range and precipitation to calculate the cloud correction factor with the MetSim package. Incoming longwave radiation is empirically derived from air temperature and relative humidity using the *Dilley and O'Brien (1998)* method. The empirical calculation method for incoming longwave radiation can be modified if desired - the `lw_clr.py` script in `summa_work/utils` provides a number of different methods to choose from. Relative humidity is empirically derived assuming the running 24 hour minimum temperature as the dewpoint for each timestep from *Running et al. (2017)*. Wind speed is set at 2 meters per second for every timestep as this is an incredibly difficult quantity to observe in mountainous regions during winter due to riming and other issues (*TODO - citation needed for this choice*). Air pressure is empirically derived using the hypsometric equation and scale height of the atmosphere for midlatitudes.\n",
    "\n",
    "The data is first pulled from the NRCS API using metloom. The data is then preprocessed to fill any missing timesteps. MetSim is then used to generate the incoming shortwave radiation. Finally, the remaining meteorological variables are calculated and converted to correct units before saving as a netcdf output file conforming to pysumma naming and formatting conventions.\n",
    "\n",
    "### How to Use\n",
    "1. In the cell below, edit desired water year in cell below\n",
    "2. Edit SNOTEL station ID, can look up on NRCS National Weather and Climate Center's Interactive Map\n",
    "3. Edit outgoing file name - no required format, whatever you choose\n",
    "4. Edit outgoing path where you would like the met forcing file for pysumma runs\n",
    "5. Create the following directories for calculated SW and snotel raw data to be stored: `./input/`, `./output`, and `./snotel_csvs/`\n",
    "6. Run all! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Clinton Alden**\n",
    "\n",
    "**Mountain Hydrology Research Group**\n",
    "\n",
    "**University of Washington**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use metloom API to pull snotel data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "from metloom.pointdata import SnotelPointData\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import xarray as xr\n",
    "from metsim import MetSim\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "from metpy.units import units\n",
    "import metpy.calc as mpcalc\n",
    "import math\n",
    "import scipy\n",
    "from pytz import UTC\n",
    "import os\n",
    "\n",
    "from utils import lw_clr\n",
    "from utils import forcing_filler as ff\n",
    "from utils import summa_check as sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide these as inputs in the future\n",
    "snotel = '1259:WA:SNTL'\n",
    "snotel_name = 'muck_WY23'\n",
    "water_year = 2023\n",
    "\n",
    "out_name = snotel_name\n",
    "# if input, output, or snotel_csvs do not exist, create it\n",
    "if not os.path.exists('./input/'):\n",
    "    os.makedirs('./input/')\n",
    "if not os.path.exists('./output/'):\n",
    "    os.makedirs('./output/')\n",
    "if not os.path.exists('./snotel_csvs/'):\n",
    "    os.makedirs('./snotel_csvs/')\n",
    "# if ./model/forcings/ doesn't exist, create it\n",
    "if not os.path.exists('./model/forcings/'):\n",
    "    os.makedirs('./model/forcings/')\n",
    "out_path = './model/forcings/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (4049223152.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[3], line 2\u001b[0;36m\u001b[0m\n\u001b[0;31m    start_year = water_year - 1\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "water_year_str = str(water_year)\\\n",
    "start_year = water_year - 1\n",
    "start_year_str = str(start_year)\n",
    "\n",
    "start_date = datetime(start_year, 7, 3)\n",
    "end_date = datetime(water_year, 9, 30)\n",
    "\n",
    "spinstart = pd.to_datetime(start_year_str + '-07-03').tz_localize('UTC')\n",
    "spinend = pd.to_datetime(start_year_str + '-09-30').tz_localize('UTC')\n",
    "\n",
    "start_loc = datetime(start_year, 10, 1).replace(tzinfo=UTC)\n",
    "mask_date = datetime(start_year, 10, 2).replace(tzinfo=UTC)\n",
    "\n",
    "dates = pd.date_range('10/01/' + start_year_str, '09/30/' + water_year_str)\n",
    "\n",
    "spin_range = pd.date_range('07/03/' + start_year_str, '09/30/' + start_year_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull desired variables from snotel to dataframe\n",
    "snotel_point = SnotelPointData(snotel, \"MyStation\")\n",
    "df = snotel_point.get_hourly_data(\n",
    "    start_date, end_date,\n",
    "    [snotel_point.ALLOWED_VARIABLES.PRECIPITATIONACCUM, snotel_point.ALLOWED_VARIABLES.TEMP, \n",
    "     snotel_point.ALLOWED_VARIABLES.SWE, snotel_point.ALLOWED_VARIABLES.SNOWDEPTH]\n",
    ")\n",
    "\n",
    "# Specify latitude, longitude, and elevation from station metadata\n",
    "lat = snotel_point.metadata.y\n",
    "lon = snotel_point.metadata.x\n",
    "elev = snotel_point.metadata.z\n",
    "\n",
    "# Clean up the dataframe\n",
    "df.reset_index(inplace=True)\n",
    "\n",
    "# Rename columns\n",
    "replace = {'ACCUMULATED PRECIPITATION':'accppt','AIR TEMP':'airtemp', 'datetime':'time'}\n",
    "df.rename(columns=replace, inplace=True)\n",
    "df.set_index('time', inplace=True)\n",
    "\n",
    "df.to_csv('./snotel_csvs/'+out_name+'.csv')\n",
    "df.drop(columns=['site', 'ACCUMULATED PRECIPITATION_units', 'geometry', 'AIR TEMP_units', 'datasource', \n",
    "                 'SWE', 'SWE_units', 'SNOWDEPTH', 'SNOWDEPTH_units'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fill missing timesteps from snotel data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the index of the dataframe to a DatetimeIndex\n",
    "df.index = pd.to_datetime(df.index)\n",
    "\n",
    "# Create a date range from the first to the last timestep\n",
    "date_range = pd.date_range(start=df.index.min(), end=df.index.max(), freq='h')\n",
    "\n",
    "# Find the missing dates\n",
    "missing_dates = date_range[~date_range.isin(df.index)]\n",
    "\n",
    "# Print the missing dates\n",
    "# print(missing_dates)\n",
    "\n",
    "# Reindex the data DataFrame with the missing dates\n",
    "# Concatenate the original DataFrame with a DataFrame containing the missing dates\n",
    "new_df = pd.concat([df, pd.DataFrame(index=missing_dates)], axis=0)\n",
    "\n",
    "# Sort the new DataFrame by the index\n",
    "new_df = new_df.sort_index()\n",
    "df = new_df\n",
    "\n",
    "# Fill NaNs for every other column\n",
    "df = df.fillna(np.nan)\n",
    "\n",
    "# Rename index\n",
    "df.index.name = 'time'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unit Conversions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Covert air temperature to celsius\n",
    "df['airtemp'] = (df['airtemp'] - 32) * 5.0/9.0\n",
    "\n",
    "# Convert precipitation to mm\n",
    "df['accppt'] = df['accppt'] * 25.4\n",
    "\n",
    "# Convert from geodataframe to dataframe\n",
    "df = pd.DataFrame(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split up data into spinup state and desired date range for MetSim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpolate the missing values\n",
    "df.interpolate(inplace=True)\n",
    "\n",
    "# Seperate the data into two dataframes, before and after October 1\n",
    "# spinstart = pd.to_datetime('2014-07-03').tz_localize('UTC')\n",
    "# spinend = pd.to_datetime('2014-09-30').tz_localize('UTC')\n",
    "spinup = df.loc[spinstart:spinend].copy()\n",
    "data = df.loc[start_loc:]\n",
    "\n",
    "# Copy the dataframe a2 to a2_copy\n",
    "data_copy = data.copy()\n",
    "\n",
    "# Create a mask to identify rows where the index is less than or equal to October 2, 2023\n",
    "mask = data_copy.index <= mask_date\n",
    "\n",
    "# Set the 'precip_accum' column to 0 for rows that satisfy the mask condition\n",
    "data_copy.loc[mask, 'accppt'] = 0\n",
    "\n",
    "# Update the value of a2 to the modified copy\n",
    "data = data_copy\n",
    "\n",
    "# Calculate the difference between the maximum value of 'precip_accum' and the previous value\n",
    "spinup['pptrate'] = spinup['accppt'].cummax().diff()\n",
    "data['pptrate'] = data['accppt'].cummax().diff()\n",
    "\n",
    "# Drop accppt column\n",
    "spinup.drop(columns=['accppt'], inplace=True)\n",
    "data.drop(columns=['accppt'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate SW from MetSim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create empty dataset\n",
    "shape = (len(dates), 1, 1, )\n",
    "dims = ('time', 'lat', 'lon', )\n",
    "\n",
    "# We are running only one site, at these coordinates\n",
    "lats = [lat]\n",
    "lons = [lon]\n",
    "elev = elev # meters\n",
    "coords = {'time': dates, 'lat': lats, 'lon': lons}\n",
    "\n",
    "# Create the initial met data input data structure\n",
    "met_data = xr.Dataset(coords=coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for varname in ['prec', 't_min', 't_max']:\n",
    "    met_data[varname] = xr.DataArray(data=np.full(shape, np.nan),\n",
    "                                     coords=coords, dims=dims,\n",
    "                                     name=varname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resample the data to daily frequency and calculate the maximum and minimum temperatures\n",
    "tmax_vals = data['airtemp'].resample('D').max()\n",
    "tmin_vals = data['airtemp'].resample('D').min()\n",
    "\n",
    "# Calculate the daily precipitation values\n",
    "prec_vals = data['pptrate'].resample('D').sum()\n",
    "\n",
    "# Interpolate the temperature values to fill in any missing days\n",
    "# tmax_vals = tmax_vals.interpolate(method='linear')\n",
    "# tmin_vals = tmin_vals.interpolate(method='linear')\n",
    "\n",
    "met_data['prec'].values[:, 0, 0] = prec_vals\n",
    "\n",
    "# Assign the daily maximum and minimum temperatures to the met_data xarray, converting to Celsius\n",
    "met_data['t_min'].values[:, 0, 0] = tmin_vals\n",
    "met_data['t_max'].values[:, 0, 0] = tmax_vals\n",
    "\n",
    "met_data.to_netcdf('./input/rc_forcing.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We form the domain in a similar fashion\n",
    "# First, by creating the data structure\n",
    "coords = {'lat': lats, 'lon': lons}\n",
    "domain = xr.Dataset(coords=coords)\n",
    "domain['elev'] = xr.DataArray(data=np.full((1,1,), np.nan),\n",
    "                          coords=coords,\n",
    "                          dims=('lat', 'lon', ))\n",
    "domain['mask'] = xr.DataArray(data=np.full((1,1,), np.nan),\n",
    "                          coords=coords,\n",
    "                          dims=('lat', 'lon', ))\n",
    "\n",
    "# Add the data\n",
    "domain['elev'][0, 0] = elev\n",
    "domain['mask'][0, 0] = 1\n",
    "domain.to_netcdf('./input/rc_domain.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, we create the state file - the dates are 90 days prior to \n",
    "# the MetSim run dates - as usual, create an empty data structure to\n",
    "# read the data into\n",
    "shape = (len(spin_range), 1, 1, )\n",
    "dims = ('time', 'lat', 'lon', )\n",
    "coords = {'time': spin_range, 'lat': lats, 'lon': lons}\n",
    "state = xr.Dataset(coords=coords)\n",
    "for varname in ['prec', 't_min', 't_max']:\n",
    "    state[varname] = xr.DataArray(data=np.full(shape, np.nan),\n",
    "                               coords=coords, dims=dims,\n",
    "                               name=varname)\n",
    "    \n",
    "# Resample precip to daily\n",
    "prec_vals = spinup['pptrate'].resample('D').sum()\n",
    "\n",
    "# Resample the data to daily frequency and calculate the maximum and minimum temperatures\n",
    "tmax_vals = spinup['airtemp'].resample('D').max()\n",
    "tmin_vals = spinup['airtemp'].resample('D').min()\n",
    "\n",
    "# Do precip data\n",
    "state['prec'].values[:, 0, 0] = prec_vals\n",
    "\n",
    "# And now temp data and convert to C\n",
    "state['t_min'].values[:, 0, 0] = tmin_vals\n",
    "state['t_max'].values[:, 0, 0] = tmax_vals\n",
    "state.to_netcdf('./input/rc_state.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dates = pd.date_range('10/01/2014', '09/30/2015')\n",
    "params = {\n",
    "    'time_step'    : \"60\",       \n",
    "    'start'        : dates[0],\n",
    "    'stop'         : dates[-1],\n",
    "    'forcing'      : './input/rc_forcing.nc',     \n",
    "    'domain'       : './input/rc_domain.nc',\n",
    "    'state'        : './input/rc_state.nc',\n",
    "    'forcing_fmt'  : 'netcdf',\n",
    "    'out_dir'      : './output',\n",
    "    'out_prefix': out_name,\n",
    "    'scheduler'    : 'threading',\n",
    "    'chunks'       : \n",
    "        {'lat': 1, 'lon': 1},\n",
    "    'forcing_vars' : \n",
    "        {'prec' : 'prec', 't_max': 't_max', 't_min': 't_min'},\n",
    "    'state_vars'   : \n",
    "        {'prec' : 'prec', 't_max': 't_max', 't_min': 't_min'},\n",
    "    'domain_vars'  : \n",
    "        {'elev': 'elev', 'lat': 'lat', 'lon': 'lon', 'mask': 'mask'}\n",
    "    }               \n",
    "\n",
    "ms = MetSim(params)\n",
    "ms.run()\n",
    "output = ms.open_output().load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create SUMMA forcing netCDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_df = output.to_dataframe()\n",
    "out_df.reset_index(inplace=True)\n",
    "out_df.set_index('time', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove timezone from index\n",
    "data.index = data.index.tz_convert(None)\n",
    "\n",
    "# Convert precipitation rate from m hr^-1 to kg m^-2 s^-1\n",
    "data['pptrate'] = data['pptrate']/3600\n",
    "\n",
    "# Generate relative humidity assuming T_d is overnight low temperature\n",
    "# Used to calculate specific humidity and longwave radiation\n",
    "ff.fill_rel_hum(data)\n",
    "\n",
    "# Convert airtemp to Kelvin\n",
    "data['airtemp'] = (1.03*(data['airtemp']-0.9)) + 273.15 # Currier snotel temp correction\n",
    "\n",
    "# Generate pressure from hypsometric equation and site elevation (1981m)\n",
    "ff.fill_pressure(data, elev)\n",
    "\n",
    "# Generate specific humidity\n",
    "ff.fill_spec_hum(data)\n",
    "data['spechum'] = data['spechum'].clip(lower=0.001)\n",
    "\n",
    "\n",
    "# Set shortwave radiation to MetSim output\n",
    "data['SWRadAtm'] = out_df['shortwave']\n",
    "\n",
    "# Generate longwave radiation\n",
    "data['LWRadAtm'] = lw_clr.dilleyobrien1998(data['airtemp'], data['rh'])\n",
    "\n",
    "# Can alternatively use the MetSim LW radiation\n",
    "# data['LWRadAtm'] = out_df['longwave']\n",
    "\n",
    "# Set wind to 2 m/s\n",
    "data['windspd'] = 2\n",
    "\n",
    "# Fill in missing values\n",
    "data['pptrate'] = data['pptrate'].fillna(0)\n",
    "\n",
    "# Drop unnecessary columns\n",
    "data = data.drop(columns=['rh'])\n",
    "\n",
    "# Interpolate the missing values\n",
    "data.interpolate(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load template forcing file to preserve attributes\n",
    "template = xr.open_dataset('./summa_forcing_template.nc')\n",
    "\n",
    "# Convert dataframe to xarray\n",
    "dsx = data.to_xarray()\n",
    "\n",
    "# Loop through variables and add attributes from template forcing file\n",
    "for data_var in dsx:\n",
    "    dsx[data_var].attrs = template[data_var].attrs\n",
    "    \n",
    "# Add hru dimension\n",
    "dsx = dsx.expand_dims(dim={'hru':1})\n",
    "\n",
    "# Add gap-filled and datastep variables\n",
    "dsx['gap_filled'] = xr.DataArray(np.ones((1,dsx.time.shape[0])),dims = ['hru','time'])\n",
    "dsx['data_step'] = 3600 # 3600 seconds for 1hr timesteps\n",
    "\n",
    "# Transpose gap filled variable to match dimensions with the rest\n",
    "# dsx['gap_filled'] = dsx['gap_filled'].T\n",
    "\n",
    "# Convert all to float64\n",
    "for var in dsx.data_vars:\n",
    "    dsx[var] = dsx[var].astype(np.float64)\n",
    "\n",
    "# Set hruID based on template\n",
    "dsx['hruId'] = (xr.DataArray(np.ones((1))*template['hruId'].values,dims = ['hru'])).astype(np.int32)\n",
    "\n",
    "# Transpose all variables to match SUMMA dimensions\n",
    "count = 0\n",
    "for var in dsx.data_vars:\n",
    "    # print(var,count)\n",
    "    count += 1\n",
    "    if count <= 7:\n",
    "        attribs = dsx[var].attrs\n",
    "        arr_t = dsx[var].values.T\n",
    "        dsx[var] = xr.DataArray(dims = ['time','hru'],data = arr_t)\n",
    "        dsx[var].attrs = attribs\n",
    "\n",
    "# Set encoding for the time variable\n",
    "# dsx['time'].encoding = {'_FillValue': np.nan, 'units': 'hours since 1990-01-01', 'calendar': 'proleptic_gregorian'}\n",
    "\n",
    "# Set hruID based on template\n",
    "dsx['hruId'] = (xr.DataArray(np.ones((1))*template['hruId'].values,dims = ['hru'])).astype(np.float64).fillna(0).astype(np.int32)\n",
    "\n",
    "dsx.to_netcdf(out_path+out_name+'.nc',\n",
    "                        encoding = {\"time\":\n",
    "                                        {'dtype' : 'float64',\n",
    "                                         'units' : 'hours since 1990-01-01 00:00:00',\n",
    "                                         'calendar' : 'standard'}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "simStartTime: \n",
      "2022-10-01T00:00:00.000000000\n",
      "simEndTime: \n",
      "2023-09-30T08:00:00.000000000\n"
     ]
    }
   ],
   "source": [
    "# print the start and end time of dsx\n",
    "print('simStartTime: ')\n",
    "print(dsx['time'].min().values)\n",
    "print('simEndTime: ')\n",
    "print(dsx['time'].max().values)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
